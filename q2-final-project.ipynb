{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 30 15:02:51 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.120                Driver Version: 550.120        CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1050 Ti     Off |   00000000:01:00.0 Off |                  N/A |\n",
      "| 30%   38C    P0             N/A /   75W |     301MiB /   4096MiB |     18%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Quadro P620                    Off |   00000000:03:00.0  On |                  N/A |\n",
      "| 34%   46C    P0             N/A /  N/A  |      74MiB /   2048MiB |     21%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2687      G   /usr/lib/xorg/Xorg                            114MiB |\n",
      "|    0   N/A  N/A      2981      G   /usr/bin/gnome-shell                           39MiB |\n",
      "|    0   N/A  N/A      3018      G   /opt/teamviewer/tv_bin/TeamViewer              22MiB |\n",
      "|    0   N/A  N/A      3613      G   /usr/libexec/xdg-desktop-portal-gnome          10MiB |\n",
      "|    0   N/A  N/A      4334      G   ...erProcess --variations-seed-version        108MiB |\n",
      "|    1   N/A  N/A      2687      G   /usr/lib/xorg/Xorg                             69MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from torchinfo import summary\n",
    "!nvidia-smi\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images\n",
    "    transforms.RandomHorizontalFlip(),  # Data augmentation\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "folderPath = \"/home/atlas/Documents/ImageNet/Kaggle/\"\n",
    "class ImageCSVLoader(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform \n",
    "        Allfolders =  os.listdir(folderPath + \"ILSVRC/Data/CLS-LOC/train\")\n",
    "        # print(Allfolders)\n",
    "        self.class_map = {label: idx for idx, label in enumerate(Allfolders)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # Total number of images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.data.iloc[idx, 0])\n",
    "        newLabel = img_name.replace(self.img_dir, '')\n",
    "        if \"train\" in self.img_dir:\n",
    "            image = Image.open(self.img_dir + newLabel.split(\"_\")[0] + \"/\" + newLabel + \".JPEG\").convert(\"RGB\")\n",
    "        else:\n",
    "            image = Image.open(img_name + \".JPEG\").convert(\"RGB\")  # Load image\n",
    "        label = str(self.data.loc[self.data[\"ImageId\"] == newLabel, \"PredictionString\"].values[0].split()[0])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.tensor(self.class_map[label], dtype=torch.long)\n",
    "\n",
    "val_dataset = ImageCSVLoader(csv_file=folderPath + \"LOC_val_solution.csv\", img_dir= folderPath + 'ILSVRC/Data/CLS-LOC/val/', transform=transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True, pin_memory=True)\n",
    "\n",
    "train_dataset = ImageCSVLoader(csv_file=folderPath + \"LOC_train_solution.csv\", img_dir=folderPath + 'ILSVRC/Data/CLS-LOC/train/', transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SeBlock(nn.Module):\n",
    "    def __init__(self, in_channels, r=16):\n",
    "        super().__init__()\n",
    "        C = in_channels\n",
    "        self.globpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(C, C // r, bias=False)\n",
    "        self.fc2 = nn.Linear(C // r, C, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [N, C, H, W]\n",
    "        f = self.globpool(x)\n",
    "        f = torch.flatten(f, 1)\n",
    "        f = self.relu(self.fc1(f))\n",
    "        f = self.sigmoid(self.fc2(f))\n",
    "        f = f[:,:,None,None]  # Adds the singleton dimensions\n",
    "        # f shape: [N, C, 1, 1]\n",
    "        scale = x * f\n",
    "        return scale\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        # SE Block for each projection\n",
    "        \n",
    "        \n",
    "        # Different projections with SE block\n",
    "        self.p1 = nn.Sequential(*[nn.Conv2d(in_channels, out_channels[0], kernel_size=1, padding=0, stride=1), self.relu, \n",
    "                                  SeBlock(out_channels[0], reduction)])\n",
    "        self.p2 = nn.Sequential(*[nn.Conv2d(in_channels, out_channels[1], kernel_size=1, padding=0, stride=1), self.relu, \n",
    "                                  nn.Conv2d(out_channels[1], out_channels[2], kernel_size=3, padding=1, stride=1), self.relu,\n",
    "                                  SeBlock(out_channels[2], reduction)])\n",
    "        self.p3 = nn.Sequential(*[nn.Conv2d(in_channels, out_channels[3], kernel_size=1, padding=0, stride=1), self.relu, \n",
    "                                  nn.Conv2d(out_channels[3], out_channels[4], kernel_size=5, padding=2, stride=1), self.relu,\n",
    "                                  SeBlock(out_channels[4], reduction)])\n",
    "        self.p4 = nn.Sequential(*[nn.MaxPool2d(kernel_size=3, padding=1, stride=1), nn.Conv2d(in_channels, out_channels[5], kernel_size=1, padding=0, stride=1),\n",
    "                                  SeBlock(out_channels[5], reduction)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        o1 = self.p1(x)\n",
    "        # print(o1.shape)\n",
    "        o2 = self.p2(x)\n",
    "        # print(o2.shape)\n",
    "        o3 = self.p3(x)\n",
    "        # print(o3.shape)\n",
    "        o4 = self.p4(x)\n",
    "        # print(o4.shape)\n",
    "        # Apply SE block to each projection\n",
    "        # o1 = self.se_block(o1)\n",
    "        # o2 = self.se_block(o2)\n",
    "        # o3 = self.se_block(o3)\n",
    "        # o4 = self.se_block(o4)\n",
    "        # print(o1.shape, o2.shape, o3.shape, o4.shape)\n",
    "        return torch.cat((o1, o2, o3, o4), axis=1)\n",
    "\n",
    "class AuxClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, classes):\n",
    "        super().__init__()\n",
    "        in_features = 4 * 4 * 128\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=5, stride=3)\n",
    "        self.conv1x1 = nn.Conv2d(in_channels=in_channels, out_channels=128, kernel_size=1, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(in_features=in_features, out_features=in_features)\n",
    "        self.fc2 = nn.Linear(in_features=in_features, out_features=classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.7)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.relu(self.conv1x1(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu(self.dropout(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class GoogLeNet(nn.Module): \n",
    "    def __init__(self, in_depth=3, classes=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        in_channels = [192, 256, 480, 512, 512, 512, 528, 832, 832, 1024]\n",
    "        feature_maps = [[64, 96, 128, 16, 32, 32],\n",
    "                        [128, 128, 192, 32, 96, 64],\n",
    "                        [192, 96, 208, 16, 48, 64],\n",
    "                        [160, 112, 224, 24, 64, 64],\n",
    "                        [128, 128, 256, 24, 64, 64],\n",
    "                        [112, 144, 288, 32, 64, 64],\n",
    "                        [256, 160, 320, 32, 128, 128],\n",
    "                        [256, 160, 320, 32, 128, 128],\n",
    "                        [384, 192, 384, 48, 128, 128]\n",
    "                    ]\n",
    "    \n",
    "        self.AuxClass1 = AuxClassifier(512, classes)\n",
    "        self.AuxClass2 = AuxClassifier(528,classes)\n",
    "        self.Blocks = nn.ModuleList([InceptionBlock(in_channels[i], feature_maps[i]) for i in range(len(feature_maps))])\n",
    "        \n",
    "        # Rest of the model\n",
    "        self.Conv7k = nn.Conv2d(in_channels=in_depth, out_channels=64, kernel_size=7, stride=2, padding=3)\n",
    "        self.Conv1k = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=1, stride=1, padding=0)\n",
    "        self.Conv3k = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=3, stride=1, padding=1)\n",
    "        self.MaxPool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.LocalNorm = nn.LocalResponseNorm(size=5)\n",
    "        self.AvgPool = nn.AvgPool2d(kernel_size=7, stride=1)\n",
    "        self.FC = nn.Linear(1024, classes)\n",
    "        self.Dropout = nn.Dropout(0.4)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        # x shape : [batch, 3, 224, 224]\n",
    "        x = self.relu(self.Conv7k(x))\n",
    "        x = self.LocalNorm(self.MaxPool(x))\n",
    "        x = self.relu(self.Conv1k(x))\n",
    "        x = self.LocalNorm(self.relu(self.Conv3k(x)))\n",
    "        x = self.MaxPool(x)\n",
    "        for i, block in enumerate(self.Blocks):\n",
    "            if i == 2 or i == 7:\n",
    "                x = self.MaxPool(x)\n",
    "            elif i == 3:\n",
    "                outputs.append(self.AuxClass1(x))\n",
    "            elif i == 6:\n",
    "                outputs.append(self.AuxClass2(x))\n",
    "\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.Dropout(self.AvgPool(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.FC(x)\n",
    "        outputs.append(x)\n",
    "        return outputs\n",
    "Inception = GoogLeNet()\n",
    "outs = Inception(torch.rand(1, 3, 224, 224))\n",
    "for out in outs:\n",
    "    print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Training]:   5%|â–Œ         | 449/8509 [24:55<7:32:00,  3.36s/it, loss=11.0857]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model and move it to device (GPU if available)\n",
    "model = GoogLeNet().to(device)\n",
    "\n",
    "# Print model summary\n",
    "# print(summary(model, input_size=(3, 224, 224)))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)  # Move loss function to GPU\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loss weighting (as used in the original Inception paper)\n",
    "aux_weight = 0.3  # Auxiliary classifiers contribute 30% to total loss\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    train_loader_tqdm = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\", leave=False)\n",
    "\n",
    "    for batch in train_loader_tqdm:\n",
    "        images, labels = batch  # Extract images and labels\n",
    "        images, labels = images.to(device), labels.to(device)  # Move images and labels to the device\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        aux1, aux2, main_out = outputs\n",
    "\n",
    "        # Compute losses\n",
    "        loss_main = criterion(main_out, labels)\n",
    "        loss_aux1 = criterion(aux1, labels) * aux_weight\n",
    "        loss_aux2 = criterion(aux2, labels) * aux_weight\n",
    "\n",
    "        total_loss = loss_main + loss_aux1 + loss_aux2\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "        train_loader_tqdm.set_postfix(loss=f\"{total_loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    val_loader_tqdm = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader_tqdm:\n",
    "            images, labels = batch  # Extract images and labels\n",
    "            images, labels = images.to(device), labels.to(device)  # Move images and labels to the device\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            aux1, aux2, main_out = outputs\n",
    "\n",
    "            # Compute losses\n",
    "            loss_main = criterion(main_out, labels)\n",
    "            loss_aux1 = criterion(aux1, labels) * aux_weight\n",
    "            loss_aux2 = criterion(aux2, labels) * aux_weight\n",
    "\n",
    "            total_loss = loss_main + loss_aux1 + loss_aux2\n",
    "            val_loss += total_loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(main_out, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            val_loader_tqdm.set_postfix(loss=f\"{total_loss.item():.4f}\")\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}, Val Accuracy = {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Save model weights after each epoch\n",
    "    torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pth')\n",
    "\n",
    "print(\"Training complete! Model weights saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'modelCustom_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 4225553,
     "sourceId": 6799,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
